{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb torch_geometric gpytorch\n",
        "!pip install --upgrade gdown"
      ],
      "metadata": {
        "id": "uT_yIbUH46eR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the four parts of the datasets in case of using the notebook from a different google drive\n",
        "# import gdown\n",
        "# gdown.download_folder(\"https://drive.google.com/drive/folders/1-JkyT8aKr9-eC2Pkof6D_qJdASYAXpUj?usp=sharing\")\n",
        "# gdown.download_folder(\"https://drive.google.com/drive/folders/1-qmcIKbazhHGkb8gRFAjHxDzi0Jv33T-?usp=sharing\")\n",
        "# gdown.download_folder(\"https://drive.google.com/drive/folders/106k3sK6wrcDzE_6ewGSUKqVD4nXlrIPE?usp=sharing\")\n",
        "# gdown.download_folder(\"https://drive.google.com/drive/folders/10HjEkProRtYt5ROzamS7SY94JVuuBOxy?usp=sharing\")"
      ],
      "metadata": {
        "id": "P3pzd4QB2XGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"./drive\")"
      ],
      "metadata": {
        "id": "qEXNojNgaPze",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "026dad6c-2cd8-4a7e-fc9a-6829fc5a725a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at ./drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\"\"\"\n",
        "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/create_dataset.html\n",
        "\"\"\"\n",
        "from torch_geometric.data  import InMemoryDataset\n",
        "\n",
        "class WeekGraphs(InMemoryDataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super().__init__(root, transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self):\n",
        "        return 'data.pt'\n",
        "\n",
        "    def process(self):\n",
        "        torch.save(self.collate(self.data_list), self.processed_paths[0])\n",
        "# dataset = WeekGraphs(\"/content/drive/MyDrive/Stock Market Prediction Graduation Project/graph_dataset_1st_year\", week_graphs)\n"
      ],
      "metadata": {
        "id": "9AJZuhDpZnRa"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_points = []\n",
        "for i in range(1,4):\n",
        "  dataset = WeekGraphs(f\"./drive/MyDrive/bloomberg_graph_{i}\")\n",
        "  for k in range(len(dataset)):\n",
        "    all_points.append(dataset[k])"
      ],
      "metadata": {
        "id": "tqD5EKmOc_Fq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.utils import (\n",
        "    add_self_loops,\n",
        "    negative_sampling,\n",
        "    remove_self_loops ,\n",
        ")\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score,average_precision_score\n",
        "from sklearn.metrics import precision_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import time\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import scipy.sparse as sp\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "# import networkx as nx\n",
        "import wandb\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch_geometric.data import DataLoader as GraphDataLoader\n",
        "from typing import List , Dict,Tuple\n",
        "from torch_geometric.nn import SAGEConv, to_hetero\n",
        "import torch\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torch_geometric.nn as  gnn\n",
        "from torch_geometric.data  import Data\n",
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from torch.optim.adamw import AdamW\n",
        "import time\n",
        "from argparse import ArgumentParser\n",
        "import argparse\n",
        "import wandb\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import datetime\n",
        "from tqdm import tqdm\n",
        "generator = torch.manual_seed(232)\n",
        "np.random.seed(232)\n",
        "torch.cuda.manual_seed(232)\n",
        "torch.cuda.manual_seed_all(232)\n",
        "random.seed(232)\n",
        "from gpytorch.models import ApproximateGP\n",
        "from gpytorch.variational import CholeskyVariationalDistribution\n",
        "from gpytorch.variational import VariationalStrategy\n",
        "import gpytorch\n",
        "from gpytorch.models import ExactGP\n",
        "from gpytorch.likelihoods import DirichletClassificationLikelihood, SoftmaxLikelihood\n",
        "from gpytorch.means import ConstantMean\n",
        "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "KaLyYJ43zyA0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size=1, num_layers=2,hidden_size=64, output_size=64, num_steps =15 ):\n",
        "        super().__init__()\n",
        "        self.lstm1 = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        # self.lstm2 = nn.LSTM(input_size=hidden_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_steps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # h0 = torch.zeros(2, x.size(0), 100).to(device) # num_layers * num_directions, batch_size, hidden_size\n",
        "        # c0 = torch.zeros(2, x.size(0), 100).to(device)\n",
        "        x = self.batch_norm1(x)\n",
        "        out, _ = self.lstm1(x)\n",
        "        # out, _ = self.lstm2(out)\n",
        "        out = F.relu(self.fc1(out[:, -1, :]))\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "6UdRLRRgnUab"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GConv(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim:int=64, num_layers:int=2, encode:bool=False, concat_out:bool=False, device='cpu', dropout=0.2):\n",
        "\n",
        "        super(GConv,self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.gconv_layers = []\n",
        "        self.norm_layers = []\n",
        "        self.encode = encode\n",
        "        for _ in range(num_layers):\n",
        "            self.gconv_layers.append(gnn.TransformerConv(emb_dim, emb_dim, heads=2, concat=False, dropout=dropout, add_self_loops = True).to(device)) # project=True ()\n",
        "            if self.encode:\n",
        "                self.norm_layers.append(nn.LayerNorm(emb_dim).to(device))\n",
        "        self.gconv_layers = nn.ModuleList(self.gconv_layers)\n",
        "        self.norm_layers = nn.ModuleList(self.norm_layers)\n",
        "\n",
        "        self.concat_out = concat_out\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "\n",
        "        outs = []\n",
        "        if self.encode:\n",
        "            outs.append(self.norm_layers[0](self.gconv_layers[0](x, edge_index)))\n",
        "        else:\n",
        "            outs.append(self.gconv_layers[0](x, edge_index))\n",
        "        for i in range(1,self.num_layers):\n",
        "            if self.encode:\n",
        "                outs.append(self.norm_layers[i](self.gconv_layers[i](outs[-1], edge_index)))\n",
        "            else:\n",
        "                outs.append(self.gconv_layers[i](outs[-1], edge_index))\n",
        "        if self.concat_out:\n",
        "            return torch.cat(outs, dim = -1)\n",
        "\n",
        "        return outs[-1]\n",
        "\n",
        "\n",
        "\n",
        "class NeuroStock(nn.Module):\n",
        "\n",
        "  def __init__(self,\n",
        "               num_timeseries_features=1,\n",
        "               n_companies=617,\n",
        "               company_emb_size=32,\n",
        "               node_emb_size=64,\n",
        "               article_emb_size=768,\n",
        "               n_industries=14,\n",
        "               n_gnn_layers=3,\n",
        "               use_timeseries_only=False,\n",
        "               graph_metadata:Tuple=None):\n",
        "    super(NeuroStock, self).__init__()\n",
        "    \"\"\"\n",
        "    company node representation will be a sum of its embedding and the output of the timeseries model (in this case it's an LSTM)\n",
        "    \"\"\"\n",
        "    self.num_timeseries_features = num_timeseries_features\n",
        "    self.n_companies = n_companies\n",
        "    self.company_emb_size = company_emb_size\n",
        "    self.node_emb_size = node_emb_size\n",
        "    self.article_emb_size = article_emb_size\n",
        "    self.n_industries = n_industries\n",
        "    self.n_gnn_layers = n_gnn_layers\n",
        "    self.use_timeseries_only = use_timeseries_only\n",
        "    self.lstm = LSTM(input_size=num_timeseries_features,  hidden_size=company_emb_size, output_size=company_emb_size).to(torch.float)\n",
        "\n",
        "    if graph_metadata is None:\n",
        "      raise(\"You need to pass HeteroData.metadata()\")\n",
        "    self.company_embedding = nn.Embedding(n_companies, company_emb_size).to(torch.float)\n",
        "    self.industry_embedding = nn.Embedding(n_industries, node_emb_size).to(torch.float)\n",
        "    self.project_article = nn.Linear(article_emb_size, node_emb_size).to(torch.float)\n",
        "\n",
        "    # to_hetero transforms normal gnn aggregation layer to a heterogeneous aggregation layer\n",
        "    # https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.to_hetero_transformer.to_hetero\n",
        "    self.g_conv = gnn.to_hetero(GConv(emb_dim=node_emb_size, num_layers=n_gnn_layers), graph_metadata).to(torch.float)\n",
        "    self.classifier = nn.Linear(node_emb_size, 2).to(torch.float)\n",
        "\n",
        "  def forward(self, hetero_x:HeteroData, return_representations=False):\n",
        "    companies_timeseries = self.lstm(hetero_x[\"company_timeseries\"][:,:, -2:-1].to(torch.float))\n",
        "    if self.use_timeseries_only:\n",
        "      out = self.classifier(companies_timeseries)\n",
        "      if return_representations:\n",
        "        return out, companies_timeseries\n",
        "      return out\n",
        "\n",
        "    hetero_x[\"sentence\"].x = self.project_article(hetero_x[\"sentence\"].x.to(torch.float))\n",
        "    companies = self.company_embedding(hetero_x[\"company\"].x)\n",
        "    # print(hetero_x[\"company_timeseries\"][:,:, -2:-1].to(torch.double).shape, hetero_x[\"company_timeseries\"][:,:, -2:-1].to(torch.float).dtype)\n",
        "    # company_timeseries is of shape (n_companies*batch_size, n_days, n_features)  the features are \"open\", \"high\", \"low\", \"close\", \"volume\"\n",
        "    hetero_x[\"company\"].x = companies_timeseries + companies  #companies are in shape (n_companies*batch_size, node_emb_size)\n",
        "\n",
        "    for k in hetero_x.edge_index_dict.keys():\n",
        "      hetero_x[k].edge_index = hetero_x[k].edge_index.to(torch.int64)\n",
        "    graph = self.g_conv(hetero_x.x_dict, hetero_x.edge_index_dict)\n",
        "    out = self.classifier(graph[\"company\"])\n",
        "    if return_representations:\n",
        "      return out, graph[\"company\"]\n",
        "    return out\n",
        "\n",
        "  def compute_loss(self, out, target):\n",
        "    loss = F.cross_entropy(out, target)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def get_output(neurostock:NeuroStock, points:List[HeteroData]):\n",
        "  data_loader = GraphDataLoader(\n",
        "                points,\n",
        "                batch_size=1, shuffle=False)\n",
        "  neurostock.eval()\n",
        "  valid_losses = []\n",
        "  # continue\n",
        "  valid_outs = []\n",
        "  valid_targets = []\n",
        "  with torch.no_grad():\n",
        "      for i, batch  in enumerate(data_loader):\n",
        "        # if i > 2: break\n",
        "        batch = batch.to(device)\n",
        "        out = neurostock(batch)\n",
        "        # print(out.squeeze(-1).unsqueeze(0))\n",
        "        # print(batch[\"target\"].shape)\n",
        "        # break\n",
        "        valid_outs.append(out.unsqueeze(0).cpu().detach())\n",
        "        valid_targets.append(batch[\"target\"].unsqueeze(0).cpu().detach())\n",
        "        loss = neurostock.compute_loss(out, batch[\"target\"])\n",
        "        valid_losses.append(loss.item())\n",
        "  valid_outs = torch.cat(valid_outs).numpy()\n",
        "  valid_targets = torch.cat(valid_targets).numpy()\n",
        "  return valid_outs, valid_targets\n",
        "\n",
        "def get_output_representations(neurostock:NeuroStock, points:List[HeteroData]):\n",
        "  data_loader = GraphDataLoader(\n",
        "                points,\n",
        "                batch_size=1, shuffle=False)\n",
        "  neurostock.eval()\n",
        "  device = next(neurostock.parameters()).device\n",
        "  valid_losses = []\n",
        "  # continue\n",
        "  representations = []\n",
        "  targets = []\n",
        "  with torch.no_grad():\n",
        "      for i, batch  in enumerate(data_loader):\n",
        "        # if i > 2: break\n",
        "        batch = batch.to(device)\n",
        "        out = neurostock.forward(batch, return_representations=True)[1]\n",
        "        representations.append(out.cpu().detach())\n",
        "        targets.append(batch[\"target\"].cpu().detach())\n",
        "\n",
        "  representations = torch.cat(representations)\n",
        "  targets = torch.cat(targets)\n",
        "  return representations, targets\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zsGbydSFnH0b"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class GPModel(ApproximateGP):\n",
        "    def __init__(self, num_classes, inducing_points):\n",
        "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
        "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
        "        super(GPModel, self).__init__(variational_strategy)\n",
        "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
        "        self.covar_module = ScaleKernel(\n",
        "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
        "            batch_shape=torch.Size((num_classes,)),\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
        "\n",
        "\n",
        "\n",
        "def get_gp_model(gp_train_x, gp_train_y, )-> Tuple[ApproximateGP, DirichletClassificationLikelihood]:\n",
        "\n",
        "\n",
        "  inducing_points = gp_train_x[np.linspace(0, len(gp_train_x)-1, 500, dtype=int)]\n",
        "\n",
        "\n",
        "  likelihood = DirichletClassificationLikelihood(gp_train_y, learn_additional_noise=True, ).to(torch.double)\n",
        "  # likelihood.noise = torch.ones_like(likelihood.noise)\n",
        "\n",
        "  model = GPModel(likelihood.num_classes, inducing_points).to(torch.double)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "      model = model.cuda()\n",
        "      likelihood = likelihood.cuda()\n",
        "  return model,  likelihood\n",
        "\n",
        "def train_gp(gp:GPModel, likelihood:DirichletClassificationLikelihood, train_dataset, test_x, test_y, least_var_k=100)->float:\n",
        "  gp_optimizer = torch.optim.Adam([\n",
        "      {'params': gp.parameters()},\n",
        "      {'params': likelihood.parameters()},\n",
        "  ], lr=0.01)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "  mll = gpytorch.mlls.VariationalELBO(likelihood, gp, num_data=len(train_dataset))\n",
        "  num_epochs = 15\n",
        "  for i in tqdm(range(num_epochs)):\n",
        "      # Within each iteration, we will go over each minibatch of data\n",
        "      gp.train()\n",
        "      likelihood.train()\n",
        "\n",
        "      for x_batch, y_batch in train_loader:\n",
        "          x_batch = x_batch.to(\"cuda\").to(torch.double)\n",
        "          y_batch = y_batch.to(\"cuda\").to(torch.double)\n",
        "          gp_optimizer.zero_grad()\n",
        "\n",
        "          output = gp(x_batch)\n",
        "          loss = -mll(output, y_batch.T)\n",
        "          loss = loss.sum()\n",
        "          loss.backward()\n",
        "          gp_optimizer.step()\n",
        "\n",
        "\n",
        "  gp.eval()\n",
        "  likelihood.eval()\n",
        "  lowest_var_accs = []\n",
        "  for i in range(len(gp_test_x)// 616):\n",
        "    with torch.no_grad():\n",
        "        test_dist = gp_model(gp_test_x[i*616: (i+1)*616].to(\"cuda\").to(torch.double))\n",
        "        pred_means = test_dist.loc.detach().cpu()\n",
        "        lower_bound, upper_bound = likelihood(test_dist).confidence_region()\n",
        "\n",
        "\n",
        "    vars = [ ]\n",
        "    for lb, ub, label in zip(lower_bound.detach().cpu().T, upper_bound.detach().cpu().T, pred_means.argmax(0)):\n",
        "      vars.append(abs(lb[label])+abs(ub[label]))\n",
        "    vars = torch.tensor(vars)\n",
        "    lowest_var_acc = (pred_means.argmax(0)[vars.argsort()[:least_var_k]] == gp_test_y[vars.argsort()[:least_var_k]]).numpy().mean()\n",
        "    lowest_var_accs.append(lowest_var_acc)\n",
        "\n",
        "  return np.mean(lowest_var_accs)"
      ],
      "metadata": {
        "id": "XYCDpaQcvmxG"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wandb.init(project=\"bloomberg_graph\", name=\"test_lstm\", config=train_config)\n",
        "import os\n",
        "os.environ[\"WANDB_API_KEY\"]=\"4d60aa176a9e71fedbbaddb7ea594784ac8cc1ad\""
      ],
      "metadata": {
        "id": "2WrZxQwMPG5f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "id": "J_FEnddZRyPu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e3d3cd7-1c24-49e5-cc8a-8d7683e70122"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmirette-gp\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import wandb\n",
        "\n",
        "\"\"\"\n",
        "file_path_in_run username/project/run_id\n",
        "\"\"\"\n",
        "\n",
        "def restore_file_wandb(file_path:str, run_name:str):\n",
        "    # if not os.path.exists(\"/\".join(file_path.split(\"/\")[:-1])):\n",
        "    #     os.makedirs(\"/\".join(file_path.split(\"/\")[:-1]))\n",
        "    wandb.restore(file_path, run_path=run_name)"
      ],
      "metadata": {
        "id": "7iQ1oOmvMYCs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "restore_file_wandb(\"neurostock_lstm.pt\", \"mirette-gp/bloomberg_graph/6dsvbqqd\")\n",
        "restore_file_wandb(\"neurostock_gnn.pt\", \"mirette-gp/bloomberg_graph/915dbdpv\")"
      ],
      "metadata": {
        "id": "ie65Gg6x2I3R"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "train_config = {\n",
        "  \"n_epochs\" : 10,\n",
        "  \"step_size\" : 10,\n",
        "  \"node_emb_size\" : 128,\n",
        "  \"lr\" : 0.001,\n",
        "  \"weight_decay\" : 0.01,\n",
        "  \"step_size\" : 10,\n",
        "  \"start_day\" : 500,\n",
        "  \"train_size\" : 300,\n",
        "  \"test_interval\" : 400,\n",
        "  \"train_batch_size\": 4,\n",
        "  \"use_timeseries_only\": False,\n",
        "  \"reset_parameters\" : False,\n",
        "  \"reset_parameters_freq\" : 5,\n",
        "  \"highest_conf_k\" : 100,\n",
        "  \"model_file_name\" : \"./neurostock_gnn.pt\",\n",
        "  \"run_name\" : \"gnn_gp_benchmark_100_gp_less_inducing_points\",\n",
        "  \"project_name\":  \"bloomberg_graph_test\",\n",
        "  \"wandb_mode\" : \"online\",\n",
        "  \"gp_train_days\" : 30\n",
        "}\n",
        "wandb.finish()\n",
        "wandb.init(\n",
        "    project=train_config[\"project_name\"],\n",
        "    name=train_config[\"run_name\"],\n",
        "    config=train_config,\n",
        "    mode=train_config[\"wandb_mode\"])\n",
        "# warmup_steps=\n",
        "neurostock = NeuroStock(\n",
        "    node_emb_size=train_config[\"node_emb_size\"],\n",
        "    company_emb_size=train_config[\"node_emb_size\"],\n",
        "    use_timeseries_only=train_config[\"use_timeseries_only\"],\n",
        "    graph_metadata=all_points[0].metadata())\n",
        "\n",
        "neurostock.to('cuda')\n",
        "device = next(neurostock.parameters()).device\n",
        "optimizer =  torch.optim.AdamW(neurostock.parameters(),\n",
        "                               lr=train_config[\"lr\"],\n",
        "                               weight_decay=train_config[\"weight_decay\"] )\n",
        "accs = []\n",
        "accum_companies = []\n",
        "high_conf_avgs = []\n",
        "gp_accs = []\n",
        "# lr_scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=0.1,  total_iters=warmup_steps)\n",
        "for split_index, split in tqdm(enumerate(range(0, train_config[\"test_interval\"], train_config[\"step_size\"]))):\n",
        "\n",
        "  if train_config[\"reset_parameters\"] and (split_index + 1) % train_config[\"reset_parameters_freq\"]  == 0 :\n",
        "    for layer in neurostock.children():\n",
        "      if hasattr(layer, 'reset_parameters'):\n",
        "          layer.reset_parameters()\n",
        "\n",
        "  train_loader = GraphDataLoader(\n",
        "      all_points[train_config[\"start_day\"]+split:train_config[\"start_day\"]+split+train_config[\"train_size\"]],\n",
        "      batch_size=train_config[\"train_batch_size\"], shuffle=True)\n",
        "  test_loader = GraphDataLoader(\n",
        "      all_points[train_config[\"start_day\"]+split+train_config[\"train_size\"]:train_config[\"start_day\"]+split+train_config[\"train_size\"]+train_config[\"step_size\"]],\n",
        "      batch_size=1, shuffle=False)\n",
        "\n",
        "  for e in tqdm(range(train_config[\"n_epochs\"])):\n",
        "      train_losses= []\n",
        "      neurostock.train()\n",
        "      train_outs = []\n",
        "      train_targets = []\n",
        "      for batch  in train_loader:\n",
        "          # with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          # with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "          batch = batch.to(device)\n",
        "          out = neurostock(batch)\n",
        "          train_outs.append(out.cpu().detach().unsqueeze(0))\n",
        "          train_targets.append(batch[\"target\"].cpu().detach().unsqueeze(0))\n",
        "          loss = neurostock.compute_loss(out, batch[\"target\"])\n",
        "          optimizer.zero_grad()\n",
        "          train_losses.append(loss.item())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # lr_scheduler.step()\n",
        "\n",
        "      neurostock.eval()\n",
        "      valid_losses = []\n",
        "      # continue\n",
        "      valid_outs = []\n",
        "      valid_targets = []\n",
        "      with torch.no_grad():\n",
        "\n",
        "          for i, batch  in enumerate(test_loader):\n",
        "            # if i > 2: break\n",
        "\n",
        "            batch = batch.to(device)\n",
        "            out = neurostock(batch)\n",
        "            valid_outs.append(out.cpu().detach().unsqueeze(0))\n",
        "            valid_targets.append(batch[\"target\"].cpu().detach().unsqueeze(0))\n",
        "            loss = neurostock.compute_loss(out, batch[\"target\"])\n",
        "            valid_losses.append(loss.item())\n",
        "\n",
        "      valid_outs = torch.cat(valid_outs).numpy()\n",
        "      valid_targets = torch.cat(valid_targets).numpy()\n",
        "      acc_per_company = (valid_outs.argmax(-1) == valid_targets).mean(axis=0)\n",
        "      high_acc_companies = np.where(acc_per_company >= 0.7)[0]\n",
        "\n",
        "      valid_acc = (valid_outs.argmax(-1) == valid_targets).mean()\n",
        "      train_outs = torch.cat(train_outs).numpy()\n",
        "      train_targets = torch.cat(train_targets).numpy()\n",
        "      train_acc = (train_outs.argmax(-1) == train_targets).mean()\n",
        "      # if e == (n_epochs- 1) :\n",
        "  representations, target = get_output_representations(neurostock,\n",
        "                                                       all_points[train_config[\"start_day\"]+split+train_config[\"train_size\"]-train_config[\"gp_train_days\"]:train_config[\"start_day\"]+split+train_config[\"train_size\"]+train_config[\"step_size\"]])\n",
        "\n",
        "  gp_train_x, gp_train_y = representations[:train_config[\"gp_train_days\"]*616], target[:train_config[\"gp_train_days\"]*616]\n",
        "  gp_test_x, gp_test_y = representations[train_config[\"gp_train_days\"]*616:], target[train_config[\"gp_train_days\"]*616:]\n",
        "  gp_model, likelihood = get_gp_model(gp_train_x, gp_train_y)\n",
        "  train_dataset = TensorDataset(gp_train_x, likelihood.transformed_targets.T)\n",
        "\n",
        "  gp_uncertainty_acc = train_gp(gp_model, likelihood, train_dataset, gp_test_x, gp_test_y)\n",
        "  high_confidence_accs = []\n",
        "  for out, target in zip(valid_outs, valid_targets):\n",
        "    predicted_label = out.argmax(-1)\n",
        "    confidence = out.max(-1)\n",
        "    predicted_label = predicted_label[confidence.argsort()[-train_config[\"highest_conf_k\"]:]]\n",
        "    target = target[confidence.argsort()[-train_config[\"highest_conf_k\"]:]]\n",
        "    high_confidence_accs.append((target ==predicted_label).mean())\n",
        "  high_conf_avgs.append(sum(high_confidence_accs)/len(high_confidence_accs))\n",
        "\n",
        "  wandb.log({\n",
        "          \"train_loss\" : np.mean(train_losses),\n",
        "          \"valid_loss\" : np.mean(valid_losses),\n",
        "          \"valid_acc\" : valid_acc,\n",
        "          \"high_confidence_acc\" : sum(high_confidence_accs)/len(high_confidence_accs),\n",
        "          \"valid_n_high\" : (acc_per_company >= 0.7).sum(),\n",
        "          \"train_acc\" : train_acc,\n",
        "          \"gp_uncertainty_split_acc\": gp_uncertainty_acc,\n",
        "    })\n",
        "  gp_accs.append(gp_uncertainty_acc)\n",
        "  accs.append(valid_acc)\n",
        "  # wandb.log({\"avg_acc\": np.mean(valid_acc)})\n",
        "  accum_companies.append(high_acc_companies)\n",
        "# wandb.finish()\n",
        "\n",
        "long_range_outputs, long_range_true = get_output(neurostock, all_points[train_config[\"start_day\"]+split+train_config[\"train_size\"]+train_config[\"step_size\"]:])\n",
        "\n",
        "all_high = []\n",
        "for k in  accum_companies:\n",
        "  all_high.extend(k)\n",
        "\n",
        "torch.save(neurostock, train_config[\"model_file_name\"])\n",
        "wandb.save(train_config[\"model_file_name\"])\n",
        "\n",
        "\n",
        "wandb.log({\"avg_acc\": np.mean(accs),\n",
        "           \"high_conf_avgs\" : np.mean(high_conf_avgs),\n",
        "           \"higher_0.7_mean\": np.mean([len(h) for h in accum_companies]),\n",
        "           \"higher_0.7_std\": np.std([len(h) for h in accum_companies]),\n",
        "           \"avg_high_company_occurrence\": np.mean(pd.Series(all_high).value_counts()[:train_config[\"highest_conf_k\"]].values),\n",
        "           \"long_range_acc\" : (long_range_outputs.argmax(-1) == long_range_true).mean(),\n",
        "           \"avg_gp_acc\" : np.mean(gp_accs),\n",
        "           \"std_gp_acc\" : np.std(gp_accs),\n",
        "           })\n",
        "wandb.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XaJSUav3rFxI",
        "outputId": "b050026a-7593-477c-9def-66159bd82bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gp_uncertainty_split_acc</td><td>▅█▄▄▆▂▁▄▇</td></tr><tr><td>high_confidence_acc</td><td>▇█▂▁▃▆▇▂▅</td></tr><tr><td>train_acc</td><td>▇█▇▁▁▃▃▂▂</td></tr><tr><td>train_loss</td><td>▁▁▂██▃▄▄▃</td></tr><tr><td>valid_acc</td><td>▄▅▅▁▅█▇▇▅</td></tr><tr><td>valid_loss</td><td>▂▁▁█▁▁▁▁▁</td></tr><tr><td>valid_n_high</td><td>▃▄▄▁▃▆▅█▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gp_uncertainty_split_acc</td><td>0.707</td></tr><tr><td>high_confidence_acc</td><td>0.544</td></tr><tr><td>train_acc</td><td>0.53828</td></tr><tr><td>train_loss</td><td>0.67367</td></tr><tr><td>valid_acc</td><td>0.52451</td></tr><tr><td>valid_loss</td><td>0.68411</td></tr><tr><td>valid_n_high</td><td>121</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gnn_gp_benchmark_100_gp_less_inducing_points</strong> at: <a href='https://wandb.ai/mirette-gp/bloomberg_graph_test/runs/icmjny9t' target=\"_blank\">https://wandb.ai/mirette-gp/bloomberg_graph_test/runs/icmjny9t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230625_192318-icmjny9t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230625_194617-edv5tcq9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mirette-gp/bloomberg_graph_test/runs/edv5tcq9' target=\"_blank\">gnn_gp_benchmark_100_gp_less_inducing_points</a></strong> to <a href='https://wandb.ai/mirette-gp/bloomberg_graph_test' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mirette-gp/bloomberg_graph_test' target=\"_blank\">https://wandb.ai/mirette-gp/bloomberg_graph_test</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mirette-gp/bloomberg_graph_test/runs/edv5tcq9' target=\"_blank\">https://wandb.ai/mirette-gp/bloomberg_graph_test/runs/edv5tcq9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0it [00:00, ?it/s]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:51,  5.75s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:12<00:48,  6.11s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:41,  5.89s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:23<00:34,  5.83s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:29<00:28,  5.76s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:34<00:22,  5.70s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:40<00:17,  5.69s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:46<00:11,  5.67s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:51<00:05,  5.64s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:57<00:00,  5.77s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:28,  2.03s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:26,  2.02s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.01s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.01s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.01s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:19,  2.12s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:16,  2.11s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:14,  2.09s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:12,  2.06s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:10,  2.04s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:08,  2.03s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:06,  2.02s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:04,  2.03s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:02,  2.11s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:30<00:00,  2.06s/it]\n",
            "1it [01:29, 89.89s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:50,  5.56s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:44,  5.61s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:16<00:39,  5.63s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:22<00:33,  5.62s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:28<00:28,  5.64s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:33<00:22,  5.64s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:39<00:16,  5.61s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:45<00:11,  5.63s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:50<00:05,  5.61s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:56<00:00,  5.61s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:29,  2.09s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:27,  2.10s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.07s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.04s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.03s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.01s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:16,  2.06s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:14,  2.06s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:12,  2.07s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:10,  2.08s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:08,  2.05s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:06,  2.03s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:04,  2.02s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:02,  2.05s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:30<00:00,  2.05s/it]\n",
            "2it [02:57, 88.77s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:50,  5.65s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:45,  5.68s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:39,  5.67s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:22<00:33,  5.65s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:28<00:28,  5.62s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:33<00:22,  5.65s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:39<00:16,  5.64s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:45<00:11,  5.61s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:50<00:05,  5.64s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:56<00:00,  5.64s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:28,  2.01s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:26,  2.01s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.03s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.06s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.08s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.05s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:16,  2.08s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:14,  2.05s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:12,  2.03s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:10,  2.02s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:08,  2.04s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:24<00:06,  2.06s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:26<00:04,  2.06s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:28<00:02,  2.04s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:30<00:00,  2.05s/it]\n",
            "3it [04:26, 88.49s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:50,  5.60s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:44,  5.61s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:16<00:39,  5.57s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:22<00:33,  5.60s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:28<00:28,  5.62s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:33<00:22,  5.60s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:39<00:16,  5.61s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:44<00:11,  5.64s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:50<00:05,  5.62s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:56<00:00,  5.63s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:28,  2.07s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:26,  2.06s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.06s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.06s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.06s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.09s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:17,  2.19s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:15,  2.16s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:19<00:12,  2.13s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:21<00:10,  2.10s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:23<00:08,  2.08s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:25<00:06,  2.07s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:27<00:04,  2.14s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:30<00:02,  2.39s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:33<00:00,  2.21s/it]\n",
            "4it [05:57, 89.49s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:06<00:55,  6.16s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:14<00:58,  7.35s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:20<00:48,  6.92s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:27<00:41,  6.89s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:34<00:34,  6.88s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:40<00:26,  6.51s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:46<00:18,  6.27s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:51<00:12,  6.05s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:57<00:05,  5.92s/it]\u001b[A\n",
            "100%|██████████| 10/10 [01:02<00:00,  6.29s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:28,  2.05s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:26,  2.05s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.05s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.04s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.07s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.09s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:17,  2.16s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:14,  2.12s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:12,  2.10s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:20<00:10,  2.08s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:22<00:08,  2.07s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:25<00:06,  2.08s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:27<00:04,  2.10s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:29<00:02,  2.11s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:31<00:00,  2.10s/it]\n",
            "5it [07:32, 91.62s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:50,  5.66s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:45,  5.75s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:40,  5.74s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:22<00:34,  5.73s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:28<00:28,  5.75s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:34<00:23,  5.79s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:40<00:17,  5.79s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:46<00:11,  5.79s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:51<00:05,  5.76s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:57<00:00,  5.77s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:29,  2.10s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:27,  2.08s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.07s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.06s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.06s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.08s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:17,  2.17s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:15,  2.16s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:18<00:12,  2.12s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:21<00:10,  2.10s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:23<00:08,  2.09s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:25<00:06,  2.08s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:27<00:04,  2.09s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:29<00:02,  2.11s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:31<00:00,  2.12s/it]\n",
            "6it [09:03, 91.32s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:51,  5.76s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:46,  5.81s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:40,  5.83s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:23<00:35,  5.94s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:29<00:29,  5.95s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:35<00:23,  5.90s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:41<00:17,  5.89s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:47<00:11,  5.88s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:52<00:05,  5.84s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:58<00:00,  5.87s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:30,  2.17s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:27,  2.11s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.08s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.08s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.07s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.07s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:16,  2.10s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:17<00:15,  2.21s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:19<00:12,  2.17s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:21<00:10,  2.13s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:23<00:08,  2.11s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:25<00:06,  2.09s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:27<00:04,  2.09s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:29<00:02,  2.10s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:31<00:00,  2.13s/it]\n",
            "7it [10:35, 91.49s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:52,  5.84s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:47,  5.89s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:41,  5.91s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:23<00:35,  5.89s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:29<00:29,  5.92s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:35<00:23,  5.91s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:41<00:17,  5.88s/it]\u001b[A\n",
            " 80%|████████  | 8/10 [00:47<00:11,  5.91s/it]\u001b[A\n",
            " 90%|█████████ | 9/10 [00:53<00:05,  5.88s/it]\u001b[A\n",
            "100%|██████████| 10/10 [00:58<00:00,  5.89s/it]\n",
            "\n",
            "  0%|          | 0/15 [00:00<?, ?it/s]\u001b[A\n",
            "  7%|▋         | 1/15 [00:02<00:30,  2.16s/it]\u001b[A\n",
            " 13%|█▎        | 2/15 [00:04<00:27,  2.12s/it]\u001b[A\n",
            " 20%|██        | 3/15 [00:06<00:24,  2.08s/it]\u001b[A\n",
            " 27%|██▋       | 4/15 [00:08<00:22,  2.07s/it]\u001b[A\n",
            " 33%|███▎      | 5/15 [00:10<00:20,  2.06s/it]\u001b[A\n",
            " 40%|████      | 6/15 [00:12<00:18,  2.05s/it]\u001b[A\n",
            " 47%|████▋     | 7/15 [00:14<00:16,  2.08s/it]\u001b[A\n",
            " 53%|█████▎    | 8/15 [00:16<00:15,  2.19s/it]\u001b[A\n",
            " 60%|██████    | 9/15 [00:19<00:12,  2.16s/it]\u001b[A\n",
            " 67%|██████▋   | 10/15 [00:21<00:10,  2.13s/it]\u001b[A\n",
            " 73%|███████▎  | 11/15 [00:23<00:08,  2.10s/it]\u001b[A\n",
            " 80%|████████  | 12/15 [00:25<00:06,  2.08s/it]\u001b[A\n",
            " 87%|████████▋ | 13/15 [00:27<00:04,  2.07s/it]\u001b[A\n",
            " 93%|█████████▎| 14/15 [00:29<00:02,  2.08s/it]\u001b[A\n",
            "100%|██████████| 15/15 [00:31<00:00,  2.11s/it]\n",
            "8it [12:06, 91.61s/it]\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [00:05<00:52,  5.87s/it]\u001b[A\n",
            " 20%|██        | 2/10 [00:11<00:47,  5.89s/it]\u001b[A\n",
            " 30%|███       | 3/10 [00:17<00:41,  5.92s/it]\u001b[A\n",
            " 40%|████      | 4/10 [00:23<00:35,  5.91s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [00:29<00:29,  5.92s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [00:35<00:23,  5.92s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [00:41<00:17,  5.90s/it]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just for testing #######################################\n",
        "# gp_optimizer = torch.optim.Adam([\n",
        "#       {'params': gp_model.parameters()},\n",
        "#       {'params': likelihood.parameters()},\n",
        "#   ], lr=0.01)\n",
        "# train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "\n",
        "# mll = gpytorch.mlls.VariationalELBO(likelihood, gp_model, num_data=len(train_dataset))\n",
        "# num_epochs = 5\n",
        "# for i in tqdm(range(num_epochs)):\n",
        "#     # Within each iteration, we will go over each minibatch of data\n",
        "#     gp_model.train()\n",
        "#     likelihood.train()\n",
        "\n",
        "#     for x_batch, y_batch in train_loader:\n",
        "#         x_batch = x_batch.to(\"cuda\").to(torch.float32)\n",
        "#         y_batch = y_batch.to(\"cuda\").to(torch.float32)\n",
        "#         gp_optimizer.zero_grad()\n",
        "\n",
        "#         output = gp_model(x_batch)\n",
        "#         # print(output.shape)\n",
        "#         # print(y_batch.shape)\n",
        "#         loss = -mll(output, y_batch.T)\n",
        "#         loss = loss.sum()\n",
        "#         loss.backward()\n",
        "#         gp_optimizer.step()\n",
        "\n",
        "\n",
        "# gp_model.eval()\n",
        "# likelihood.eval()\n",
        "# lowest_var_accs = []\n",
        "# for i in range(len(gp_test_x)// 616):\n",
        "#   with torch.no_grad():\n",
        "#       test_dist = gp_model(gp_test_x[i*616: (i+1)*616].to(\"cuda\").to(torch.double))\n",
        "#       pred_means = test_dist.loc.detach().cpu()\n",
        "#       lower_bound, upper_bound = likelihood(test_dist).confidence_region()\n",
        "\n",
        "\n",
        "#   vars = [ ]\n",
        "#   for lb, ub, label in zip(lower_bound.detach().cpu().T, upper_bound.detach().cpu().T, pred_means.argmax(0)):\n",
        "#     vars.append(abs(lb[label])+abs(ub[label]))\n",
        "#   vars = torch.tensor(vars)\n",
        "#   lowest_var_acc = (pred_means.argmax(0)[vars.argsort()[:100]] == gp_test_y[vars.argsort()[:100]]).numpy().mean()\n",
        "#   lowest_var_accs.append(lowest_var_acc)\n",
        "# np.mean(lowest_var_accs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYSEqzLSlVxM",
        "outputId": "13266c14-2d06-44ac-cdb8-1ec6af237a1b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:14<00:00,  2.83s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.724"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}